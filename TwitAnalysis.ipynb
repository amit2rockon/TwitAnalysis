{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense ,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amit/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "training_data= np.genfromtxt('Data/Sentiment Analysis Dataset.csv', delimiter=',',skip_header = 1, usecols = (1,3) , dtype = None)\n",
    "train_x = [str(x[1]) for x in training_data]\n",
    "train_y = np.asarray([y[0] for y in training_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words= 3000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "dictionary = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('dictionary.json','w') as dictionary_file: # saving the dictionary file to use it later\n",
    "  json.dump(dictionary,dictionary_file)\n",
    "  \n",
    "def text_to_index_array(text):\n",
    "  return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "\n",
    "for text in train_x:\n",
    "  words = text_to_index_array(text)\n",
    "  allWordIndices.append(words)\n",
    "\n",
    "  \n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices,mode ='binary')\n",
    "train_y = keras.utils.to_categorical(train_y,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/amit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/amit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 1420764 samples, validate on 157863 samples\n",
      "Epoch 1/5\n",
      "1420764/1420764 [==============================] - 1747s 1ms/step - loss: 0.4947 - acc: 0.7613 - val_loss: 0.4488 - val_acc: 0.7896\n",
      "Epoch 2/5\n",
      "1420764/1420764 [==============================] - 1993s 1ms/step - loss: 0.4736 - acc: 0.7759 - val_loss: 0.4499 - val_acc: 0.7904\n",
      "Epoch 3/5\n",
      "1420764/1420764 [==============================] - 1854s 1ms/step - loss: 0.4661 - acc: 0.7818 - val_loss: 0.4456 - val_acc: 0.7910\n",
      "Epoch 4/5\n",
      "1420764/1420764 [==============================] - 1875s 1ms/step - loss: 0.4608 - acc: 0.7856 - val_loss: 0.4469 - val_acc: 0.7927\n",
      "Epoch 5/5\n",
      "1420764/1420764 [==============================] - 1944s 1ms/step - loss: 0.4571 - acc: 0.7885 - val_loss: 0.4456 - val_acc: 0.7936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1835a39eb8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512,input_shape=(max_words,),activation ='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256,activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics =['accuracy'])\n",
    "model.fit(train_x,train_y,batch_size=32,epochs=5,verbose=1,validation_split=0.1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.json','w') as json_file:\n",
    "  json_file.write(model_json)\n",
    "model.save_weights('twit_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mData\u001b[m\u001b[m/               dictionary.json     readme.md           useit.py\r\n",
      "TwitAnalysis.ipynb  model.json          twit_model.h5\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
